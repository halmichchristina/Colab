{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "W_GPU_WGAIN_V2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halmichchristina/Colab/blob/master/W_GPU_WGAIN_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip2uc6LXSsth",
        "colab_type": "text"
      },
      "source": [
        "# GAIN GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ8labrJuzcg",
        "colab_type": "text"
      },
      "source": [
        "## Needed Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4_JO-pYSstn",
        "colab_type": "code",
        "outputId": "ce3a8cbe-cfe5-4f34-de9e-6dc9d34f3519",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "import torchvision.transforms as transforms\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.tensor as tensor\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import torch.optim as optim\n",
        "#Gradient Penalty\n",
        "import torch.autograd as autograd\n",
        "\n",
        "#GPU\n",
        "!pip install --upgrade pycuda\n",
        "import pycuda.driver as cuda \n",
        "import pycuda.autoinit # Necessary for using its functions\n",
        "#Time\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: pycuda in /usr/local/lib/python3.6/dist-packages (2019.1.2)\n",
            "Requirement already satisfied, skipping upgrade: mako in /usr/local/lib/python3.6/dist-packages (from pycuda) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (4.4.0)\n",
            "Requirement already satisfied, skipping upgrade: pytools>=2011.2 in /usr/local/lib/python3.6/dist-packages (from pycuda) (2019.1.1)\n",
            "Requirement already satisfied, skipping upgrade: appdirs>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pycuda) (1.4.3)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from mako->pycuda) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.8.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from pytools>=2011.2->pycuda) (1.16.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZAzwZj19-Pp",
        "colab_type": "text"
      },
      "source": [
        "## GPU Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDlPv5tMmc-3",
        "colab_type": "code",
        "outputId": "272ff6fb-f448-4d02-8501-793ba39f4aa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "# A simple class to know about your cuda devices\n",
        "cuda.init() # Necesarry for using its functions\n",
        "\n",
        "class aboutCudaDevices():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    \n",
        "    def num_devices(self):\n",
        "        \"\"\"Return number of devices connected.\"\"\"\n",
        "        return cuda.Device.count()\n",
        "    \n",
        "    def devices(self):\n",
        "        \"\"\"Get info on all devices connected.\"\"\"\n",
        "        num = cuda.Device.count()\n",
        "        print(\"%d device(s) found:\"%num)\n",
        "        for i in range(num):\n",
        "            print(cuda.Device(i).name(), \"(Id: %d)\"%i)\n",
        "            \n",
        "    def mem_info(self):\n",
        "        \"\"\"Get available and total memory of all devices.\"\"\"\n",
        "        available, total = cuda.mem_get_info()\n",
        "        print(\"Available: %.2f GB\\nTotal:     %.2f GB\"%(available/1e9, total/1e9))\n",
        "        \n",
        "    def attributes(self, device_id=0):\n",
        "        \"\"\"Get attributes of device with device Id = device_id\"\"\"\n",
        "        return cuda.Device(device_id).get_attributes()\n",
        "    \n",
        "    def __repr__(self):\n",
        "        \"\"\"Class representation as number of devices connected and about them.\"\"\"\n",
        "        num = cuda.Device.count()\n",
        "        string = \"\"\n",
        "        string += (\"%d device(s) found:\\n\"%num)\n",
        "        for i in range(num):\n",
        "            string += ( \"    %d) %s (Id: %d)\\n\"%((i+1),cuda.Device(i).name(),i))\n",
        "            string += (\"          Memory: %.2f GB\\n\"%(cuda.Device(i).total_memory()/1e9))\n",
        "        return string\n",
        "\n",
        "# You can print output just by typing its name (__repr__):\n",
        "print(aboutCudaDevices())\n",
        "# 1 device(s) found:\n",
        "#    1) Tesla K80 (Id: 0)\n",
        "#          Memory: 12.00 GB\n",
        "\n",
        "# Returns the current GPU memory usage by \n",
        "# tensors in bytes for a given device\n",
        "print(torch.cuda.memory_allocated())\n",
        "\n",
        "# Returns the current GPU memory managed by the\n",
        "# caching allocator in bytes for a given device\n",
        "print(torch.cuda.memory_cached())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "1 device(s) found:\n",
            "    1) Tesla K80 (Id: 0)\n",
            "          Memory: 12.00 GB\n",
            "\n",
            "0\n",
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPsjzT_PSsuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#next(D.parameters()).is_cuda # Ceck if mod is on GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9qTudTb-6hM",
        "colab_type": "text"
      },
      "source": [
        "## Generator and Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnSVHJLCSsuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Generator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x_in, m, z): #hier als x bereits bearbeitete\n",
        "        inp =(1-m)*z# noise for missing values \n",
        "        input_x = torch.cat((x_in,m,inp),dim = 1)\n",
        "        x = F.relu(self.map1(input_x.cuda()))\n",
        "        x = F.relu(self.map2(x))\n",
        "        x = torch.tanh(self.map3(x))\n",
        "        return x\n",
        "      \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.map1 = nn.Linear(input_size, hidden_size)\n",
        "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.map3 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x_h, h):\n",
        "        input_x = torch.cat((x_h,h),dim = 1)\n",
        "        x = F.relu(self.map1(input_x.cuda()))\n",
        "        x = F.relu(self.map2(x))\n",
        "        x = torch.sigmoid(self.map3(x))\n",
        "        #output in WGAN = 1\n",
        "        # x =  torch.sigmoid(self.map3(x))\n",
        "        return x "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD2bibkeSsuL",
        "colab_type": "text"
      },
      "source": [
        "## Sampling of M, Z , H"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1D-K3mVSsuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mask Vector\n",
        "def sample_M(samples, n, p):\n",
        "    A = torch.from_numpy(np.random.uniform(0., 1., size = [samples, n])).double()\n",
        "    B = A > p\n",
        "    C = 1.*B  \n",
        "    return C.to(device)\n",
        "\n",
        "# Random sample generator for Z\n",
        "def sample_Z(samples, n):\n",
        "    return torch.from_numpy(np.random.uniform(0., 1., size = [samples, n])).double()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka2xtIrV_7FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hint for fake values -> Set 0.5 for one 0\n",
        "def get_fake_H(M):\n",
        "    H_fake = M.clone()\n",
        "    samples, n = H_fake.shape\n",
        "    m_index = torch.arange(samples, dtype=torch.long)\n",
        "    n_index = m_index.clone()    \n",
        "    for mi in m_index:  \n",
        "        pos = (M[mi] == 0).nonzero()\n",
        "        n_i= torch.from_numpy(np.asanyarray(np.random.choice(pos[:,0])))\n",
        "        H_fake[mi, n_i] = 0.5       \n",
        "    indices = zip(m_index, n_index)\n",
        "    return H_fake.to(device), indices\n",
        "\n",
        "  \n",
        "  \n",
        "# Hint for real values -> Set 0.5 for one 1\n",
        "def get_real_H(M):\n",
        "    H_fake = M.clone()\n",
        "    samples, n = H_fake.shape\n",
        "    m_index = torch.arange(samples, dtype=torch.long)\n",
        "    n_index = m_index.clone()    \n",
        "    for mi in m_index:  \n",
        "        pos = (M[mi] == 1).nonzero()\n",
        "        n_i= torch.from_numpy(np.asanyarray(np.random.choice(pos[:,0])))\n",
        "        H_fake[mi, n_i] = 0.5       \n",
        "    indices = zip(m_index, n_index)\n",
        "    return H_fake.to(device), indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D38pRYmNu764",
        "colab_type": "text"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDOl8TZ_Ssuo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_data(p, dim, input_dim, batch, print_ex):\n",
        "    # X, M , Z\n",
        "    root = './data'\n",
        "    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
        "    train_set = torchvision.datasets.MNIST(root=root, train=True, transform=trans, download=True)\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_set,batch_size=batch, shuffle=True)\n",
        "    \n",
        "    \n",
        "    # get some random training images\n",
        "    dataiter = iter(train_loader)\n",
        "    images, labels = dataiter.next()\n",
        "    images = images.to(device)\n",
        "    x = images.view(-1,input_dim)\n",
        "     \n",
        "    m = sample_M(batch, input_dim, p)\n",
        "    z = sample_Z(batch, input_dim)\n",
        "    #show example\n",
        "    if print_ex == 1: \n",
        "        \n",
        "        exX = x.view(-1,1,dim,dim)\n",
        "        exM = m.view(-1,1,dim,dim).cpu().double()\n",
        "        imshow(torchvision.utils.make_grid(exX[0]))\n",
        "        imshow(torchvision.utils.make_grid(exM[0]))\n",
        "        #im =exX[0]*exM[0]\n",
        "        #imshow(torchvision.utils.make_grid(im))\n",
        "        print(x.shape)\n",
        "        print(z.shape)\n",
        "        print(m.shape)\n",
        "        \n",
        "    return x.float().to(device),m.float().to(device),z.float().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGIGV_qmvasW",
        "colab_type": "text"
      },
      "source": [
        "## Loss, Show, GP Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RVyrTjNSsuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img, title):\n",
        "    fig, ax = plt.subplots(figsize=(15,5))\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.cpu().numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fswwr43Y6mBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_gradient_penalty(D, x_real, x_s,h):\n",
        "    #Where x_s is the output of the Generator\n",
        "    a = torch.rand(batch, 1)\n",
        "    a = a.expand(x_real.size())\n",
        "    a = a.to(device)\n",
        "    #print(\"# DEBUG:::a.shape = \" + str(a.shape))\n",
        "    #print(\"# DEBUG:::x_real.shape = \" + str(x_real.shape))\n",
        "    #print(\"# DEBUG:::x_s.shape = \" + str(x_s.shape))\n",
        "    interpolates = a * x_real + ((1 - a) * x_s)\n",
        "    interpolates = interpolates.to(device)\n",
        "    interpolates = autograd.Variable(interpolates, requires_grad=True)\n",
        "    out = D(interpolates,h)\n",
        "  \n",
        "    gradients = autograd.grad(outputs=out, inputs=interpolates,\n",
        "                              grad_outputs=torch.ones(out.size()).to(device),\n",
        "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
        "\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA\n",
        "    return gradient_penalty\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBKH-pvDzFUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_MSE(calc_x,real_x,m):\n",
        "  samples, n = m.shape\n",
        "  m_index = torch.arange(samples, dtype=torch.long)\n",
        "  res= 0\n",
        "  for mi in m_index:  \n",
        "    print('mi', mi)\n",
        "    pos = (m[mi] == 1).nonzero()\n",
        "    #print('pos', pos)\n",
        "    res += torch.sum(calc_x[pos] - real_x[pos])**2\n",
        "  return res.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnn7f9F3u_04",
        "colab_type": "text"
      },
      "source": [
        "## RUN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe0lJveGvIHW",
        "colab_type": "text"
      },
      "source": [
        "### Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ssEXMv1Ssu0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Varaibles Run\n",
        "num_epochs =5000\n",
        "p_miss = 0.2\n",
        "batch = 150\n",
        "dim = 28\n",
        "\n",
        "#Parameters\n",
        "d_learning_rate = 1e-6  \n",
        "g_learning_rate = 1e-4  \n",
        "g_sgd_momentum = 0.2\n",
        "d_sgd_momentum = 0.9\n",
        "e = 1E-8\n",
        "alpha = 10\n",
        "LAMBDA = 10 #Gradient Penalty\n",
        "\n",
        "#Variables Net\n",
        "input_dim = 784\n",
        "g_hidden_size = 392\n",
        "g_output_size = 784\n",
        "d_hidden_size = 392\n",
        "d_output_size = 1\n",
        "\n",
        "\n",
        "\n",
        "#Nets\n",
        "G = Generator(input_size=input_dim*3, hidden_size= g_hidden_size,output_size=g_output_size)\n",
        "D = Discriminator(input_size=input_dim*2, hidden_size=d_hidden_size, output_size=d_output_size)\n",
        "G = G.to(device)\n",
        "D = D.to(device)\n",
        "\n",
        "#Loss for real observed and computed observed\n",
        "L_M = nn.MSELoss().cuda()\n",
        "\n",
        "#Optimize\n",
        "\n",
        "#adam geht auch gut\n",
        "d_optimizer = optim.RMSprop(D.parameters(), lr=d_learning_rate)# , weight_decay =d_sgd_momentum)\n",
        "g_optimizer = optim.RMSprop(G.parameters(), lr=g_learning_rate)# ,momentum=g_sgd_momentum)\n",
        "\n",
        "#Error List\n",
        "plot_rmse,plot_G,plot_D = [],[],[]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrYHvNxMvwZv",
        "colab_type": "text"
      },
      "source": [
        "### Get Image for Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IV74dRnHCqT",
        "colab_type": "code",
        "outputId": "808827b5-e098-464e-8a2c-2abf59989349",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        }
      },
      "source": [
        "x,m,z = prepare_data(p_miss, dim, input_dim, batch,0)\n",
        "moh = x\n",
        "moh_x = x.view(-1,1,dim,dim)\n",
        "bla = moh_x[0]\n",
        "imshow(torchvision.utils.make_grid(bla), 'Original')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE/CAYAAAAnhFRiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WDOHLCIv2Yf",
        "colab_type": "text"
      },
      "source": [
        "### GAIN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "pwMw2vEmSsvy",
        "colab_type": "code",
        "outputId": "fd266843-c963-4fa1-dadc-d42b9b5326b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        }
      },
      "source": [
        "start_complete = time.time()\n",
        "for epoch in range(num_epochs): #or while train_loss not converged do:  \n",
        "    #1.Train D with fixed G\n",
        "    D.zero_grad()\n",
        "    G.zero_grad()\n",
        "    \n",
        "    #Get Data\n",
        "    x,m,z = prepare_data( p_miss, dim, input_dim, batch,0)\n",
        "    x_in = m*x\n",
        "\n",
        "    #Run Generator\n",
        "    x_s = G(x_in.to(device), m, z)\n",
        "    x_h = m*x_in+(1-m)*x_s\n",
        "    \n",
        "    #Get H for fake and real\n",
        "    h_real, indices_real = get_real_H(m.cpu())\n",
        "    h_fake, indices_fake = get_fake_H(m.cpu())\n",
        "    \n",
        "    #Train Only Discriminator\n",
        "    m_h_real = D(x_h.to(device),h_real.to(device))\n",
        "    m_h_fake = D(x_h.to(device),h_fake.to(device))\n",
        "      \n",
        "    \n",
        "    #Calculate Gradient Penalty\n",
        "    gradient_penalty = calc_gradient_penalty(D, x, x_s,h_fake) #h_real oder h_fake?\n",
        "    \n",
        "    D.zero_grad()\n",
        "    \n",
        "    \n",
        "    # E_real so groß wie möglich E_fake so klein wie möglich\n",
        "    D_loss = torch.mean(m_h_fake)-torch.mean(m_h_real) + gradient_penalty\n",
        "\n",
        "    \n",
        "    D_loss.backward() \n",
        "    d_optimizer.step()     \n",
        "        \n",
        "    \n",
        "\n",
        "                        #Gentrennt optimiern\n",
        "                        #m_h_real.backward()\n",
        "                        #m_h_real.step()\n",
        "                        #m_h_fake.backward()\n",
        "                        #m_h_fake.step()\n",
        "\n",
        "\n",
        "  #2.Train G on D's response (but DO NOT train D on these labels)\n",
        "    G.zero_grad()\n",
        "    D.zero_grad()\n",
        "    \n",
        "    #Get Data\n",
        "    x,m,z = prepare_data( p_miss, dim, input_dim, batch,0)\n",
        "    x_in = m*x\n",
        "    \n",
        "    #Run Generator\n",
        "    x_s = G(x_in.to(device), m, z)\n",
        "    x_h = m*x_in+(1-m)*x_s\n",
        "    \n",
        "    \n",
        "    #Get H for fake and real\n",
        "    h_real, indices_real = get_real_H(m.cpu())\n",
        "    h_fake, indices_fake = get_fake_H(m.cpu())\n",
        "    \n",
        "    #Train Only Discriminator\n",
        "    m_h_real = D(x_h.to(device),h_real.to(device))\n",
        "    m_h_fake = D(x_h.to(device),h_fake.to(device))\n",
        "    \n",
        "        \n",
        "    #Train only Generator first x with real x to capture info\n",
        "    #LM = L_M(x_s,x_in)\n",
        "    LM = calc_MSE(x_s,x_in,m)\n",
        "    print(\"# DEBUG::: L;M =  \" + str(LM))\n",
        "\n",
        "       \n",
        "    \n",
        "    # Then calculate loss of imputed now we use Wasserstein       \n",
        "    LG = -torch.mean(m_h_fake) #(torch.mean(LG_real)  he can't change the true ones\n",
        "    \n",
        "    G_loss = LG + alpha * LM\n",
        "    \n",
        "    G_loss.backward()\n",
        "    g_optimizer.step()\n",
        "    \n",
        "       \n",
        "    \n",
        "    #PRINT \n",
        "    plot_rmse.append(torch.sqrt(LM.detach()))\n",
        "    plot_G.append(G_loss.detach())\n",
        "    plot_D.append(D_loss.detach())\n",
        "    if epoch % 1000 == 0:\n",
        "      print('Epoch: ',epoch)\n",
        "      \n",
        "      #Print always the same picture\n",
        "      bla_in = m*moh.float()\n",
        "      bla_s1 = G(bla_in, m, z)\n",
        "      exxs = bla_s1.detach()\n",
        "      bla_s= exxs.view(-1,1,dim,dim)\n",
        "      bla_h = m*bla_in+(1-m)*bla_s1\n",
        "      exxh = bla_h.detach()\n",
        "      bla_h= exxh.view(-1,1,dim,dim)      \n",
        "      im = torch.cat((bla,bla_s[0]),dim = 1)\n",
        "      im = torch.cat((im,bla_h[0]),dim = 1)\n",
        "\n",
        "      a = str(epoch)\n",
        "      a = 'Epoch: '+a\n",
        "\n",
        "      imshow(torchvision.utils.make_grid(im), a)\n",
        "      \n",
        "end_complete = time.time()\n",
        "print('Run complete:',(end_complete-start_complete)/60)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mi tensor(0)\n",
            "mi tensor(1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b00930d57188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m#Train only Generator first x with real x to capture info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m#LM = L_M(x_s,x_in)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mLM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_MSE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_s\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_in\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# DEBUG::: L;M =  \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-763c59bbf417>\u001b[0m in \u001b[0;36mcalc_MSE\u001b[0;34m(calc_x, real_x, m)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mmi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mm_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m#print('pos', pos)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalc_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreal_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: copy_if failed to synchronize: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chXZ0NwySswH",
        "colab_type": "text"
      },
      "source": [
        "## Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtBMI_xXwi4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rmse =[]\n",
        "for i in range(len(plot_rmse)):\n",
        "  rmse.append(plot_rmse[i].cpu().numpy())\n",
        "\n",
        "rmse = np.asarray(rmse)\n",
        "\n",
        "\n",
        "g_loss=[]\n",
        "for i in range(len(plot_G)):\n",
        "  g_loss.append(plot_G[i].cpu().numpy())\n",
        "\n",
        "g_loss = np.asarray(g_loss)\n",
        "\n",
        "\n",
        "d_loss=[]\n",
        "for i in range(len(plot_D)):\n",
        "  d_loss.append(plot_D[i].cpu().numpy())\n",
        "\n",
        "d_loss = np.asarray(d_loss)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZouSYYU261J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "rmse_n = (rmse-min(rmse))/(max(rmse)-min(rmse))\n",
        "g_loss_n = (g_loss-min(g_loss))/(max(g_loss)-min(g_loss))\n",
        "d_loss_n =(d_loss-min(d_loss))/(max(d_loss)-min(d_loss))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,5))\n",
        "ax.plot(g_loss_n, 'r', label='G loss')\n",
        "ax.plot(d_loss_n, 'b', label='D loss')\n",
        "ax.plot(rmse_n, 'g', label='RMSE')\n",
        "\n",
        "legend = ax.legend(loc='upper right',  fontsize='x-large')\n",
        "plt.title('MNIST')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP416Iqsw6D7",
        "colab_type": "text"
      },
      "source": [
        "## Mean RMSE and STD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxsXgug2wNJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "error = []\n",
        "for i in range(10):\n",
        "    x,m,z = prepare_data( p_miss, dim, input_dim, batch,0)\n",
        "    x_s = G(x_in, m, z)\n",
        "    LM = L_M(x_s,x_in)\n",
        "    error.append(torch.sqrt(LM.detach()))\n",
        "res = torch.mean(torch.stack(error))\n",
        "print(\"RMSE\",res)\n",
        "\n",
        "std = torch.std(torch.stack(error))\n",
        "print(\"STD\",std)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHnFeMBIxQmj",
        "colab_type": "text"
      },
      "source": [
        "## GPU memory release"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7jE6OO_8NvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(torch.cuda.memory_allocated())\n",
        "print(torch.cuda.memory_cached())\n",
        "\n",
        "# Releases all unoccupied cached memory currently held by\n",
        "# the caching allocator so that those can be used in other\n",
        "# GPU application and visible in nvidia-smi\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}